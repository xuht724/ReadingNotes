# 【从链说起】聊聊以太坊智能合约执行

## 背景故事

在本篇文章开始之前，我需要简单的介绍一下现在的以太坊使用的执行机制。

众所周知，区块链上每一个区块的区块头中会记录当前世界状态的表示，在以太坊的区块结构中，这个字段被称为`stateRoot`。计算本区块的`stateRoot`的过程是区块链新块产生中至关重要的一个环节。这个过程并不复杂，可以参考下图来进行理解。

![image-20220209202939542](https://raw.githubusercontent.com/xuht724/blog-img/main/img/image-20220209202939542.png)

这里的`Previous State`指的是在没有执行本区块打包的交易的时候的世界状态。显然，计算新块状态的过程是一个顺序执行的过程：每一笔交易修改了以太坊区块链状态数的某些叶子结点进行了修改，从而导致世界状态的更新；一笔交易完成后得到的世界状态是新的交易的输入，直到本区块要打包的交易全部执行完成，就得到了最终的区块状态。需要说明的是这里只是逻辑上的表示，并不代表实际的代码实现。（事实上，在每一笔交易执行完成后只会修改合约状态（状态树的叶子节点），而不会计算全局状态。）

## 问题

这样的串行执行模型虽然简洁直观，但是也会带来一些有趣的问题：比如说这篇发布在2021年S&P上的[Paper](https://arxiv.org/pdf/2009.14021.pdf)介绍的三明治攻击。文章提到了一种在去中心化交换(DEX)中通过构造一对数量相同、方向相反的交易在目标交易先后执行，从而实现无风险套利的攻击。这种攻击能够成功的一大原因就是**矿工在打包交易的时候能够按照自己的需求对于交易排序**，从而实现额外利润获取，这部分的利润被称为**矿工可提取价值(MEV)**。在此就不牵扯太多关于MEV的研究的问题，如果对于MEV感兴趣，可以查看[Flashbots](https://docs.flashbots.net)的主页的文章。本篇文章讨论的是这种串行执行模型是否会给区块链系统带来性能或者安全上的问题。

一个直观的感受是，随着单个区块中包含的交易数的增加，矿工计算区块最终状态的时间也会线性增加，在现有主流的区块链公链中，可能这并不会造成明显的限制，但是对于我们未来想要构建的高性能公链，如果我们不改变现有的执行模型，**智能合约执行速度将会成为限制区块链网络吞吐量的核心因素之一**。事实也的确如此，根据[康奈尔大学的学者的研究](https://www.youtube.com/watch?v=2_GX8iCVNrA)，**参考现有的以太坊上的交易数据，现在的evm导致的平均吞吐量限制是900tps**。

> Tips: 这或许和现有公链系统公布的数据有所出入。的确如此，因为区块链tps本质上是一个比较模糊的指标。区块链并不是单一的分布式计算、存储系统，很难选出一个标准的transaction来作为区块链系统性能的单位。矿工在处理区块链原生代笔转账交易和复杂的DEX合约时候开销肯定是不一致的。因此在采用不同标准的时候会得到不同的结果也是正常的。只是区块链项目有时为了起到宣传效果，会强调本身设计的理论性能（往往是共识算法所能够实现的理论性能），但是在实际的场景中可能并不能打达到共识协议所带来的吞吐量。这从宣传的角度无可厚非，但是作为区块链的研究人员还是要明确这一点。

另一个能够想到的后果是：这会**阻碍新节点的加入，从而使得区块链网络更加中心化**。在现有的设计中，节点加入到区块链只会向网络上的节点下载所有区块，而不会下载状态树。节点想要获取世界状态需要从创世区块开始计算，而由于区块链状态的更迭是串行执行每一笔交易得到的，这就意味着随着区块链的增长，状态的同步将会花费更长的时间，现有研究表明，普通的计算机同步以太坊的最新的状态需要花费一周的时间，而如果是作为档案节点加入，同步状态的时间甚至无法赶上以太坊区块链的增长速度。一个解决办法是节点在加入以太坊网络的时候会向一些档案节点请求下载某个区块对应的世界状态，并从该区块开始执行，直到获取最新的世界状态。但是该办法也要求大多数的档案节点是可信的，同样使得区块链网络更加中心化。

也许我们能够做一些改变来加速现有的EVM。

## 现有研究

### 总体思路

一个直观的思路是：**矿工在执行的时候，可以尝试性的并行执行一些交易，当交易发生冲突的时候按照一定策略进行调整，最后矿工记录自己执行交易的顺序告诉所有其他的验证者，验证者根据给的交易顺序并发执行所有的交易，从而验证区块。**

事实上现有的研究也都是基于这个框架做的。这个框架下需要解决的问题有两个。

第一个问题是，矿工如何找到并处理读写冲突的交易。

另一个问题是，矿工应该传递什么给验证者，能让验证者更高效地验证区块。

让我们看看现有的一些研究。

### 悲观并发控制：锁机制

在我们编写多线程代码，访问共享资源的时候，需要给共享资源加锁，如果没有对于线程安全保护机制，那么可能会造成对于共享资源的访问冲突。而对于智能合约来说，因为所有的账户共享同一个世界状态，所有存放在Storage中的数据都是共享资源。因此，一个直观的思路是在智能合约交易要访问世界状态的时候，对于请求的资源加上锁，在发生交易冲突时，将某一交易交易移动到后续串行执行。这也是这篇发表在2017年PODC会议上的[《Adding Concurrency to Smart Contracts》](https://dl.acm.org/doi/pdf/10.1145/3087801.3087835)文章所采取的思路。如下图所示。

![image-20220215012617184](https://raw.githubusercontent.com/xuht724/blog-img/main/img/image-20220215012617184.png)

文章提出将智能合约的执行分成并发阶段和串行阶段。

在并发阶段，以太坊执行智能合约交易时，交易运行到了状态访问的指令（比如说SLOAD、SSTORE指令的时候），会对于访问的状态变量添加一个锁（可以是互斥锁也可以是读写锁），表示当前该状态已经被交易所访问，如上图的交易1，2，4。此时如果有交易也要访问该状态变量，比如交易3，需要访问`<Key2,Value>`处状态变量，因为该变量已经被交易2锁定，那么交易3就需要被移到后续的串行阶段进行执行。在串行阶段，

执行完成之后，虚拟机会生成基本的交易依赖图，该图是一个有向无环图，图上的节点代表每一个交易，边表示了交易的执行顺序，比如从节点1指向节点2的边，表明交易1需要在2之前执行。没有边相连表示交易可以并发执行。矿工需要把该图（或者其拓扑序）打包到区块中，网络中的其他验证节点可以据此运行交易，得到和矿工并发执行一样的结果。

如上图，验证者知道了验证的时候可以并发执行交易1，2，4，交易2执行完成之后再执行3。可以看到的不足是，对于验证者来说，交易3一定要在2完成之后进行执行。这是因为上述图中并没有记录交易2，3冲突的具体数据，如果记录了2和3的全部冲突数据，验证者也可以并发执行交易3，在执行到冲突状态的时候阻塞执行，直到2执行完成之后获取数据。

该文章的作者后续用真实的以太坊的交易数据来分析该执行策略能够得到的加速比。写了一篇论文放到了Arxiv上。文章使用的数据是2016年7月（DAO fork之后）到2017年12月。每个阶段持续7天，各个阶段之间间隔11周。

![image-20220128145543699](https://raw.githubusercontent.com/xuht724/blog-img/main/img/image-20220128145543699.png)

可以看到的是随着时间的推移，冲突率越高，加速比越低，并且，提升硬件的核心数也可以带来一定的加速比的提升。文章得到的一些结论如下：

1. 简单的推测的并发执行就可以实现不错的加速比
2. 随着交易流量的增加，加速比会降低
3. 区分Read和Write是很重要的，同等对待交易的read和write会显著提升交易的冲突率
4. **更激进的策略，比如说设置多个并行阶段，没有产生什么好处。**
5. 准确的静态分析可以有一定的好处
6. 提高模拟的以太坊虚拟机核的数目，从16到64个，有比较大的提升，但是超过64个之后，提升就比较小了
7. 在冲突率比较高的时期，绝大多数冲突都是因为一些主要受欢迎的合约导致的

> Tips：文章由于并没有实现真实支持并发的EVM，计算加速比的方法是采用了估算的方法，使用的两种估算策略是合约交易的gas cost和交易的operation number。这两种标准得到的结果是类似的，但是这这两种策略都无法正确的交易执行时间

文章提供的数据从16年7月到17年12月，加速比（64cores）从最高8.87的时候降低到略微超过2。主要的原因新的区块中会发生更多的数据冲突。在2017年12月份的数据中，16cores的实验中34%的交易会中止，从而达到的加速比只有1.13。

文章还提到另一个有趣的一点是，数据冲突的产生很大程度上是热门合约造成的。在去掉这段时间的热门合约（CryptoKitties）之后，能够将加速比从1.13提升到1.65。而根据我个人之前做的简单的数据分析，最近以太坊智能合约绝大多数的交易都和Uniswap以及USDC等有限的几个交易有关，直观感觉是现在区块中包含交易的data conflict可能会比当时的更多，不过具体的数据还值得更深入的探索。

### 乐观并发控制

数据库系统中并发访问控制已经被研究三十年了，总体而言，这些工作可以被分为两类，一类是悲观的并发控制，比如说前面提到的锁机制。与之对应的，还存在其他乐观并发控制的方法：**一个进程在读写共享内存的时候不需要考虑其他进程的行为和状态**，并记录日志，执行完成进行验证，如果验证通过则提交结果，若失败，则重置日志并重试。

总体而言，乐观并发控制的思路是，先并行执行区块中的所有交易，记录所有交易的读写集，**根据交易的读写集找到冲突的交易从而构建交易的依赖图，将依赖图打包进区块发送给验证者进行并发验证**。和悲观并发控制相比，**该方法构建出的交易依赖图要更加复杂**，除了包含交易的依赖关系，也**记录了交易之间具体需要传输的数据，因此能够进行更加精细的并发调度**。因此，**相关的工作更关注对于交易依赖图的处理，以及验证节点对于交易的并发验证**。

该方向比较有代表性的是华东师范大学的相关研究   [研究1](https://arxiv.org/pdf/1905.07169.pdf)  [研究2](https://ieeexplore.ieee.org/document/9356389) 。简单介绍来说，该研究主要有两部分：

1. 从交易的读写冲突图构建交易的依赖图；

   该文认为，如果最终构建的交易依赖图越稀疏，就意味着依赖图中包含的交易之间的内部 **冲突越少**，其交易 **并发度就越高**。因此图的密度属性很好地描述了并发度，因此要尽可能的密度低的交易依赖图。同时依赖图的信息要尽可能完备，例如交易1指向交易2的边信息需要记录交易2从交易1读取的数据，边的权重被设置为读取数据的大小。

2. 并发调度日志的计算：交易依赖图的划分

   该研究认为，验证节点即使有了完整的交易依赖图，由于实际资源的限制，也无法实现实现最优的并发：即为每一笔交易都会设置一个单独的线程执行。因此需要对于交易依赖图进行划分，由不同的线程执行不同的子图，同时令**不同的子图之间的边的权重尽可能小，这意味着不同线程执行的时候需要同步的信息尽可能少**。如下图所示，验证节点会在两个线程中分别按照子图的拓扑序执行。当线程2（假设左边为线程1，右边为线程2），执行到交易5的时候，需要进行阻塞，等待线程1中交易4执行完成，从交易4执行完成的结果读取数据。，类似的，对于线程1执行到交易9的时候，也需要等待线程2中交易11执行完成的结果。

   ![image-20220215152033335](https://raw.githubusercontent.com/xuht724/blog-img/main/img/image-20220215152033335.png)

### 简单总结

总体而言，现有研究已经将数据库中的悲观并发控制和乐观并发控制的方法都运用到智能合约的执行、验证的环节中来。

对比这两种方法。悲观并发控制简洁直观，在区块内部冲突率较低时，能够较好地加速矿工产生区块，但是生成的交易依赖图较为简单，验证者无法进行高效的并发验证。乐观并发控制能够提供足够详细的并发调度日志，但是额外添加了计算与划分交易依赖图的开销。

可惜的是，无论是哪一种方法，现有的研究都还是使用非合约语言、虚拟机环境做的模拟实验，都没有尝试去开发出支持并发的虚拟机，因此我们也无法得知这些策略究竟能够给合约执行带来多大的提升。

## 我们还能做什么？

以下是我个人对于这个问题的一些思考。

### 更有价值的数据分析 ✅

前文提到的那篇真实交易数据分析的论文只到了2017年12月，那时最火的合约还是加密猫。现在已经是2022年2月了，现在Eth上最火的contract是uniswap以及usdt等热门的ERC20代币合约。因此很有必要去分析现在区块中交易的依赖冲突。这有助于我们判断，在现有环境下，我们可以通过编排交易的执行顺序来实现大约多少的加速比。同时，也有必要了解现在区块中包含的交易的数目，这有助于了解对于真实区块，计算交易依赖图的开销。

### 更合理的架构设计 ✅

一个观点是，即使我们设计出了较好支持并发交易的以太坊虚拟机，Evm的吞吐量瓶颈可能也还是无法有较好提升。这是因为对于全节点来说，执行智能合约的主要开销是对于世界状态的频繁访问而造成的读写开销（实际上主要是读）。让虚拟机支持多线程并发执行，更多的是利用现代CPU的多核进行计算加速，可能对于总体的执行来说并不会有太大的改善。也许我们可以像[RainBlock](https://www.usenix.org/system/files/atc21-ponnapalli.pdf)文章中提到的，在公链中引入IO-Helper来帮助缓解合约状态获取的开销。

一个不错但不成功的尝试是发生在2021年4月15日的柏林硬分叉引入的[EIP2930](https://eips.ethereum.org/EIPS/eip-2930)。简单来说，该改动允许交易携带一个`access list`，记录本交易要访问的状态的Key值。作为激励，所有记录在的`access list`的状态变量的访问会少收取200单位的gas费用。但是可惜的是，从柏林硬分叉引入到现在的8个月以来，使用该方法节约gas cost的交易的比例简直少的可怜（后续会做具体的数据分析）。个人认为，**这是因为以太坊的用户都是轻节点，并没有自己要访问的状态变量的信息**。如果该类型的交易能够被广泛应用，一个可以预见的好处是，矿工可以提前打包不发生状态冲突的交易，而不需要自己执行生成，从而更快地并发生成交易，更早的进行挖矿。

我个人认为，区块链现有对于节点的设计存在一定的不合理：矿工完成了几乎所有的事情：搜索，验证，执行，排序交易；也垄断了近乎全部的权力：比如说矿工能够肆无忌惮地构造front-running交易和back-runnning交易——因为他们拥有着任意排列交易顺序的权力，这导致的gas费用猛涨，才导致的Flashbots这样的组织的成立来进行亡羊补牢。也许我们在设计公链的时候应该适当将原有矿工的工作进行拆分（类似Fabric的设计），从而设计出更加优秀的公链。

## 尾声

这篇文章从现有以太坊串行执行、验证交易的架构讲起，介绍了该架构可能导致的问题。并分别从悲观和乐观并发控制的角度介绍了现有研究对其的改进。最后总结了一些围绕这一问题而衍生的个人思考。解决了一点问题的同时，也牵扯出了更多的问题。以太坊是非常优秀的公链，但是它确实太早了，以至于现在看来它的设计充满不尽人意的地方。也确实应该去了解一些更多的公链，比较他们的异同，才能对于某一方面有比较深入的认识。

